---
title: TiDBベストプラクティス
summary: TiDBのベストプラクティスを学びます。
aliases: ['/docs/dev/tidb-best-practices/']
---

# TiDBベストプラクティス

このドキュメントでは、TiDBの使用に関するベストプラクティスをまとめています。特に、オンライン分析処理（OLAP）およびオンライントランザクション処理（OLTP）シナリオにおけるSQLの使用と最適化のヒント、およびTiDB固有の最適化オプションについて説明します。

このドキュメントを読む前に、以下の3つのブログ記事でTiDBの技術原理が紹介されていることをおすすめします：

* [TiDB Internal (I) - データストレージ](https://en.pingcap.com/blog/tidb-internal-data-storage/)
* [TiDB Internal (II) - コンピューティング](https://en.pingcap.com/blog/tidb-internal-computing/)
* [TiDB Internal (III) - スケジューリング](https://en.pingcap.com/blog/tidb-internal-scheduling/)

## 前書き

データベースは一般的なインフラストラクチャシステムです。開発プロセス中にさまざまなユーザーシナリオを考慮し、実際のビジネスシナリオに応じてデータパラメータや使用方法を変更することが重要です。

TiDBはMySQLプロトコルと構文に互換性のある分散データベースですが、内部実装と分散ストレージおよびトランザクションのサポートにより、TiDBの使用方法はMySQLとは異なります。

## 基本的な概念

これらのベストプラクティスは、その実装原理に密接に関連しています。Raftコンセンサスアルゴリズム、分散トランザクション、データシャーディング、負荷分散、SQLからKey-Value（KV）へのマッピングソリューション、セカンダリインデックスの実装方法、分散実行エンジンなどの基本メカニズムについて学ぶことをおすすめします。

このセクションではこれらの概念について紹介します。詳細な情報については、[PingCAPのブログ記事](https://pingcap.com/blog/)を参照してください。

### Raft

Raftはデータの強い整合性を保証するコンセンサスアルゴリズムです。TiDBはRaftを使用してデータをレプリケートします。TiDBはデータを書き込む前に、複数のレプリカにデータを書き込みます。これにより、いくつかのレプリカが失われても、システムには最新のデータが残ります。たとえば、3つのレプリカがある場合、データが2つのレプリカに書き込まれるまで、システムは成功の結果を返しません。ということは、少なくとも残りの2つのレプリカのうちの1つには最新のデータがあります。

3つのレプリカを保存するために、ソース-レプリカのレプリケーションと比較して、Raftの方が効率的です。Raftの書き込み待ち時間は最も速い2つのレプリカに依存しており、最も遅いレプリカに依存しないため、ジオ分散と複数のアクティブデータセンターの実装が可能になります。たとえば、2つのサイトに分散した3つのデータセンターの典型的なシナリオでは、データの整合性を保証するために、TiDBはすべての3つのデータセンターにデータを書き込む必要はありません。ただし、これはすべてのシナリオでクロスデータセンターデプロイメントが実装できるという意味ではありません。書き込むデータの量が多い場合、データセンター間の帯域幅とレイテンシが重要な要素になります。書き込み速度が帯域幅を超えたり、レイテンシが高すぎる場合、Raftのレプリケーションメカニズムはうまく機能しません。

### 分散トランザクション

TiDBは完全な分散トランザクションを提供し、モデルは[Google Percolator](https://research.google.com/pubs/pub36726.html)を基にいくつかの最適化が施されています。このドキュメントでは、次の機能について説明します：

* 楽観的トランザクションモデル

    TiDBの楽観的トランザクションモデルは、コミットフェーズまで衝突を検出しません。衝突がある場合、トランザクションはリトライが必要です。ただし、衝突が深刻な場合、このモデルは効率的ではありません。なぜなら、リトライ前の操作は無効であり、繰り返す必要があるからです。

    データベースがカウンターとして使用されている場合、高いアクセス同時実行性は深刻な衝突を引き起こし、複数のリトライまたはタイムアウトが発生する可能性があります。したがって、深刻な衝突が発生するシナリオでは、悲観的なトランザクションモードを使用するか、Redisなどのシステムアーキテクチャレベルで問題を解決することをおすすめします。ただし、アクセスの衝突があまり深刻でない場合、楽観的トランザクションモデルは効率的です。

* 悲観的トランザクションモード

    TiDBでは、悲観的トランザクションモードはMySQLとほぼ同じ動作をします。トランザクションは実行フェーズ中にロックを適用し、衝突する状況でリトライを回避し、より高い成功率を保証します。悲観的ロックを適用することで、`SELECT FOR UPDATE`を使用して事前にデータをロックすることもできます。

    ただし、アプリケーションシナリオに衝突が少ない場合、楽観的トランザクションモデルの方が性能が良いです。

* トランザクションサイズ制限

    分散トランザクションは2フェーズコミットを実行し、下位レイヤーでRaftレプリケーションを実行する必要があるため、トランザクションが非常に大きい場合、コミットプロセスは非常に遅くなり、次のRaftレプリケーションプロセスが停止してしまいます。この問題を回避するために、トランザクションのサイズに制限があります：

    - トランザクションは5,000個のSQLステートメントまで制限されています（デフォルト）
    - 各Key-Valueエントリはデフォルトで6 MBを超えることはありません
    - Key-Valueエントリの合計サイズは10 GBを超えることはありません。

    類似の制限は、[Google Cloud Spanner](https://cloud.google.com/spanner/quotas)でも見られます。

### データシャーディング

TiKVは、キーの範囲に基づいてデータを自動的にシャードします。各Regionはキーの範囲であり、左側は閉じられていて、右側は開いている区間`[StartKey、EndKey)`です。Region内のKey-Valueペアの数がある閾値を超えると、Regionは自動的に2つに分割されます。

### 負荷分散

Placement Driver（PD）は、全体のTiKVクラスターの状態に基づいてクラスターの負荷を均等に分散します。スケジューリングの単位はRegionであり、ロジックはPDが設定したストラテジーです。

### KV上のSQL

TiDBはSQL構造を自動的にKey-Value構造にマッピングします。詳細については、[TiDB Internal (II) - Computing](https://en.pingcap.com/blog/tidb-internal-computing/)を参照してください。

具体的には、TiDBは次の操作を実行します：

* 1行のデータは1つのKey-Valueペアにマッピングされます。キーは`TableID`で接頭辞が付けられ、行IDで接尾辞が付けられます。
* インデックスは1つのKey-Valueペアとしてマッピングされます。キーは`TableID+IndexID`で接頭辞が付けられ、インデックスの値で接尾辞が付けられます。

同じテーブル内のデータまたはインデックスは同じ接頭辞を持っています。これらのKey-Valueは、TiKVのキースペースの隣接した位置にあります。そのため、書き込むデータの量が多く、すべてのデータが1つのテーブルに書き込まれる場合、書き込みのホットスポットが作成されます。また、連続して書き込まれるデータの一部のインデックス値も連続している場合（例：`更新時間`などの時間とともに増加するフィールド）、いくつかの書き込みホットスポットが作成され、システム全体のボトルネックになります。

同様に、データが集中的な範囲からすべて読み取られる場合（例：連続した数万行または数十万行のデータ）、データのアクセスホットスポットが発生する可能性があります。

### セカンダリインデックス

TiDBは完全なセカンダリインデックス（グローバルインデックス）をサポートしています。多くのクエリはインデックスを使用して最適化することができます。そのため、アプリケーションでセカンダリインデックスを効果的に活用することが重要です。

多くのMySQLの経験はTiDBでも適用できます。ただし、TiDBには独自の特徴があります。次に、TiDBでセカンダリインデックスを使用する際のいくつかの注意点を示します。

* セカンダリインデックスが多ければよいですか？

    セカンダリインデックスはクエリの処理を高速化することができますが、インデックスの追加には副作用があります。前のセクションではインデックスのストレージモデルについて紹介しています。追加のインデックスごとに1つのKey-Valueが追加されるため、インデックスの数が多いほど書き込み速度が遅くなり、より多くのスペースを使用します。

    また、インデックスの数が多いと、オプティマイザの実行時間が長くなり、不適切なインデックスがオプティマイザを誤誘導することもあります。したがって、セカンダリインデックスを増やすことはパフォーマンスが向上することを意味しません。

* どの列にインデックスを作成するべきですか？

    上記の通り、インデックスは重要ですが、インデックスの数は適切である必要があります。アプリケーションの特性に応じて適切なインデックスを作成する必要があります。原則として、クエリに関与する列にインデックスを作成してパフォーマンスを向上させる必要があります。以下に、インデックスを作成する必要がある状況を示します：

    - 差分の大きい列に対しては、インデックスによってフィルタリングされる行が著しく減少します。
    - 複数のクエリ条件がある場合、複合インデックスを選択できます。複合インデックスの場合、同等の条件の列を先に配置することに注意してください。

    たとえば、よく使用されるクエリが `select * from t where c1 = 10 and c2 = 100 and c3 > 10`である場合、複合インデックス `Index cidx (c1, c2, c3)`を作成できます。このように、クエリ条件をインデックスの接頭辞として使用してスキャンできます。

* インデックスを使用してクエリする方法とテーブルを直接スキャンする方法の違い

    TiDBはグローバルインデックスを実装しているため、インデックスとテーブルのデータは必ずしも同じデータシャーディングにあるわけではありません。インデックスを使用してクエリする場合、まずインデックスをスキャンして対応する行IDを取得し、その後、行IDを使用してデータを取得する必要があります。つまり、この方法では2つのネットワークリクエストが行われ、ある程度のパフォーマンスオーバーヘッドが発生します。

    クエリに多数の行が含まれる場合、インデックスのスキャンは並行して行われます。最初のバッチの結果が返されると、テーブルのデータを取得できます。したがって、これは並行+パイプラインモデルです。2つのアクセスにはオーバーヘッドが発生しますが、レイテンシは高くありません。

    次の2つの条件では、2つのアクセスの問題は発生しません：
- インデックスの列がクエリ要件にすでに満たされています。`t` テーブルの `c` 列にインデックスがあり、クエリが `select c from t where c > 10;` の場合、この時点でインデックスにアクセスすれば必要なデータがすべて取得できます。この状況は「カバリングインデックス」と呼ばれます。ただし、クエリのパフォーマンスを重視する場合は、フィルタリングする必要はないが、クエリ結果として返される列の一部にインデックスを付けて、複合インデックスを作成することで最適化できます。`select c1, c2 from t where c1 > 10;` を例にとると、複合インデックス `Index c12 (c1, c2)` を作成することでこのクエリを最適化できます。

- テーブルの主キーは整数です。この場合、TiDB では主キーの値を行 ID として使用します。したがって、クエリ条件が主キーにある場合、行 ID の範囲を直接構築し、テーブルデータをスキャンして結果を取得できます。

* クエリの同時実行

  データが多くのリージョンに分散されているため、TiDB ではクエリは同時実行されます。ただし、デフォルトでは高い並行性は、多くのシステムリソースを消費する場合には使用されません。また、OLTP クエリには通常、大量のデータが関与することはなく、低い並行性で問題ありません。ただし、OLAP クエリにおいては、並行性が高く、TiDB は以下のシステム変数を使用してクエリの並行性を変更します。
  
  - [`tidb_distsql_scan_concurrency`](/system-variables.md#tidb_distsql_scan_concurrency):

      テーブルおよびインデックスデータをスキャンする場合の並行性を示します。

  - [`tidb_index_lookup_size`](/system-variables.md#tidb_index_lookup_size):

      テーブルデータにアクセスする前にインデックスにアクセスする必要がある場合、一括で複数の行 ID を単一のリクエストとして使用します。このパラメータはバッチのサイズを設定します。大きなバッチは待機時間を増加させますが、小さなバッチだとクエリが増加する可能性があります。このパラメータの適切なサイズは、クエリに関与するデータ量に関連します。一般的には、変更は必要ありません。

  - [`tidb_index_lookup_concurrency`](/system-variables.md#tidb_index_lookup_concurrency):

      テーブルデータにアクセスする前にインデックスにアクセスする必要がある場合、このパラメータを使用して毎回データを取得する並行性を変更します。

* インデックスを使用して結果の順序を確保する

  インデックスを使用してデータをフィルタリングまたは並び替えることができます。まず、インデックスの順序に従って行 ID を取得します。その後、行 ID の戻り順に基づいて行内容を返します。これにより、返される結果はインデックス列に従って順序付けられます。以前にインデックスをスキャンし、行を取得するモデルがパイプライン + 並列であることが触れられています。インデックス順に行が返される場合、2つのクエリ間の高い並行性は遅延を引き起こしません。したがって、デフォルトでは並行性は低くなっていますが、[`tidb_index_serial_scan_concurrency`](/system-variables.md#tidb_index_serial_scan_concurrency) 変数を介して変更できます。

* 逆転インデックスのスキャン

  TiDB は昇順インデックスを逆順でスキャンすることをサポートしており、通常のスキャンよりも 20% 遅い速度で進行します。データが頻繁に変更されるためバージョンが多すぎる場合、パフォーマンスのオーバーヘッドが高くなる可能性があります。逆転インデックスのスキャンは可能な限り避けることをお勧めします。

## シナリオとプラクティス

最後のセクションでは、TiDB の基本的な実装メカニズムと使用方法への影響について説明しました。このセクションでは、展開からアプリケーションの使用まで、具体的な使用シナリオと操作プラクティスについて紹介します。

### 展開

展開前に、[ソフトウェアとハードウェアの要件](/hardware-and-software-requirements.md)を読んでください。

TiDB クラスタを [TiUP](/production-deployment-using-tiup.md) を使用して展開することをお勧めします。このツールは、クラスタ全体をデプロイし、停止し、破棄し、アップグレードすることができます。これは非常に便利です。TiDB クラスタを手動で展開することはお勧めしません。後でのメンテナンスやアップグレードに面倒なことがあるかもしれません。

### データの取り込み

インポートプロセス中の書き込みパフォーマンスを向上するためには、[TiKV メモリパラメータのパフォーマンスを調整する](/tune-tikv-memory-performance.md) に記載されているように、TiKV のパラメータを調整できます。

### 書き込み

前述のように、TiDB は Key-Value レイヤの単一トランザクションのサイズを制限しています。SQL レイヤでは、1 行のデータが 1 つの Key-Value エントリにマップされます。追加のインデックスごとに 1 つの Key-Value エントリが追加されます。

> **注意:**
>
> トランザクションのサイズ制限を設定する際は、TiDB エンコーディングのオーバーヘッドと追加のトランザクションキーを考慮する必要があります。1 つのトランザクションの行数は 200 未満であること、および 1 つの行のデータサイズは 100KB 未満であることが推奨されます。そうでないと、パフォーマンスが低下する可能性があります。

`INSERT`、`UPDATE`、`DELETE` ステートメントは、ステートメントをバッチに分割するか、ステートメントに制限を追加することが推奨されます。

大量のデータを削除する場合は、`Delete from t where xx limit 5000;` を使用することをお勧めします。これにより、ループを介して削除を行い、`Affected Rows == 0` を条件としてループを終了できます。

一度に削除する必要があるデータが多い場合、このループ処理は徐々に遅くなってしまいます。なぜなら、各削除が後ろ向きにトラバースするからです。以前のデータを削除した後、しばらくの間、多くの削除済みフラグが残ります（その後、ガベージコレクションによりすべてがクリアされますが）、これが後続の `DELETE` ステートメントに影響を与えます。可能であれば、`WHERE` 条件を絞り込むことをお勧めします。たとえば、`2017-05-26` のすべてのデータを削除する必要がある場合、以下のステートメントを使用できます：

```sql
for i from 0 to 23:
    while affected_rows > 0:
        delete from t where insert_time >= i:00:00 and insert_time < (i+1):00:00 limit 5000;
        affected_rows = select affected_rows()
```

この擬似コードは、巨大なデータを小さなデータに細分化して削除し、以前の `Delete` ステートメントが後続のステートメントに影響を与えないようにするためのものです。

### クエリ

クエリ要件と具体的なステートメントについては、[システム変数](/system-variables.md)を参照してください。

`SET` ステートメントを使用して SQL 実行の並行性を制御したり、`Join` オペレーターの選択をヒントを用いて行ったりすることができます。

さらに、標準の MySQL インデックス選択やヒント構文を使用したり、オプティマイザを制御して `Use Index`/`Ignore Index` ヒントを介してインデックスを選択したりすることができます。

OLTP と OLAP 両方のワークロードが存在するアプリケーションシナリオの場合、OLTP 要求と OLAP 要求を異なる TiDB サーバーに送信し、OLTP への OLAP の影響を減らすことができます。OLAP ワークロードを処理する TiDB サーバーには、高性能のハードウェア（たとえば、より多くのプロセッサコアや大容量のメモリなど）を搭載したマシンを使用することをお勧めします。

OLTP と OLAP のワークロードを完全に分離するためには、TiFlash 上で OLAP アプリケーションを実行することをお勧めします。TiFlash は、OLAP ワークロードに非常に優れたパフォーマンスを発揮する列指向のストレージエンジンです。TiFlash はストレージ層で物理的な分離を実現し、一貫した読み取りを保証します。

### 監視とログ

システムの状態を把握する最良の方法は監視メトリクスです。TiDB クラスタと一緒に監視システムを展開することをお勧めします。

TiDB は、[Grafana + Prometheus](/tidb-monitoring-framework.md)を使用してシステム状態を監視しています。TiDB を TiUP を使用して展開すると、監視システムが自動的に展開および構成されます。

監視システムにはたくさんの項目があり、そのうちの大部分は TiDB 開発者向けです。ソースコードの深い理解がなければ、これらの項目を理解する必要はありません。アプリケーションまたはシステムの主要コンポーネントの状態に関連するいくつかの項目が選択され、別々の `overview` パネルに配置されています。

監視に加えて、システムログも表示できます。TiDB の３つのコンポーネントである tidb-server、tikv-server、pd-server それぞれに `--log-file` パラメータがあります。クラスタを起動する際にこのパラメータを設定している場合は、ログはパラメータで設定されたファイルに保存され、ログファイルは自動的に毎日アーカイブされます。`--log-file` パラメータが設定されていない場合は、ログは `stderr` に出力されます。

TiDB 4.0 以降、TiDB に [TiDB Dashboard](/dashboard/dashboard-intro.md) UI が提供され、使いやすさが向上しました。ブラウザで <http://${PD_IP}:${PD_PORT}/dashboard> を開くことで TiDB Dashboard にアクセスできます。TiDB Dashboard には、クラスタの状態の表示、パフォーマンス分析、トラフィックの視覚化、クラスタ診断、ログ検索などの機能が備わっています。

### ドキュメント

システムについて学ぶ最良の方法は、そのドキュメントを読んで実装原則を理解することです。

TiDB には多くの公式ドキュメントがあり、中国語と英語の両方で提供されています。問題に遭遇した場合は、[FAQ](/faq/tidb-faq.md) と [TiDB クラスタのトラブルシューティングガイド](/troubleshoot-tidb-cluster.md) から始めることができます。また、[TiDB リポジトリの GitHub](https://github.com/pingcap/tidb) で問題リストを検索したり、問題を作成したりすることができます。

TiDB には多くの有用な移行ツールがあります。詳細は [移行ツールの概要](/ecosystem-tool-user-guide.md) を参照してください。

TiDB の技術的な詳細に関するその他の記事については、[PingCAP 公式ブログサイト](https://pingcap.com/blog/) を参照してください。

## TiDB の最適なシナリオ

TiDB は、以下のシナリオに適しています：
- スタンドアロンデータベースにはデータのボリュームが大きすぎます
- シャーディングを行いたくありません
- アクセスモードには明白なホットスポットがありません
- トランザクション、強力な一貫性、そして災害復旧が必要です
- リアルタイムのハイブリッドトランザクション/分析処理（HTAP）アナリティクスが欲しいです、またストレージのリンクを削減したいです