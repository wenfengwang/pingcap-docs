---
title: PDスケジューリングのベストプラクティス
summary: PDスケジューリングのためのベストプラクティスと戦略を学びましょう。
aliases: ['/docs/dev/best-practices/pd-scheduling-best-practices/','/docs/dev/reference/best-practices/pd-scheduling/']
---

# PDスケジューリングのベストプラクティス

このドキュメントでは、PDスケジューリングの原則と戦略について、一般的なシナリオを通じて説明します。このドキュメントでは、以下のコアコンセプトについての基本的な理解があることを前提としています：

- [リーダー/フォロワー/ラーナー](/glossary.md#leaderfollowerlearner)
- [オペレータ](/glossary.md#operator)
- [オペレータステップ](/glossary.md#operator-step)
- [ペンディング/ダウン](/glossary.md#pendingdown)
- [リージョン/ピア/Raftグループ](/glossary.md#regionpeerraft-group)
- [リージョン分割](/glossary.md#region-split)
- [スケジューラ](/glossary.md#scheduler)
- [ストア](/glossary.md#store)

> **ノート:**
>
> このドキュメントは初めにTiDB 3.0を対象としています。一部の機能は以前のバージョン(2.x)でサポートされていない可能性がありますが、基本的なメカニズムは類似しているため、このドキュメントは引き続き参考として使用できます。

## PDスケジューリングポリシー

このセクションでは、スケジューリングシステムに関与する原則とプロセスについて紹介します。

### スケジューリングプロセス

スケジューリングプロセスは一般的に以下の3つのステップからなります：

1. 情報の収集

    各TiKVノードは定期的に2つの種類のハートビートをPDに報告します：

    - `StoreHeartbeat`：ストアの全体的な情報を含み、ディスクの容量、利用可能なストレージ、読み書きトラフィックなどが含まれます。
    - `RegionHeartbeat`：リージョンの全体的な情報を含み、各リージョンの範囲、ピアの分布、ピアの状態、データ量、読み書きトラフィックなどが含まれます。

    PDはこの情報を収集し、スケジューリングの決定に使用します。

2. オペレータの生成

    異なるスケジューラは独自のロジックと要件に基づいてオペレータを生成し、以下の考慮事項があります：

     - 異常な状態（切断、ダウン、ビジー、低スペース）のストアにはピアを追加しない
     - 異常な状態のリージョンのバランスをとらない
     - ペンディングピアにはリーダーを転送しない
     - リーダーを直接削除しない
     - 異なるリージョンピアの物理的な分離を崩さない
     - ラベルのプロパティなどの制約を守る

3. オペレータの実行

    オペレータを実行するための一般的な手順は以下の通りです：

    1. 生成されたオペレータは最初に`OperatorController`が管理するキューに参加します。

    2. `OperatorController`はキューからオペレータを取り出し、設定に基づいて一定の並行性でオペレータを実行します。このステップでは、各オペレータステップを対応するリージョンリーダーに割り当てます。

    3. オペレータは「完了」または「タイムアウト」としてマークされ、キューから削除されます。

### 負荷分散

リージョンの負荷分散を実現するために、主に`balance-leader`と`balance-region`スケジューラを使用します。両方のスケジューラはクラスタ内のすべてのストアにリージョンを均等に分散させることを目標としていますが、異なる焦点を持っています。`balance-leader`はクライアントリクエストの均等な分散を実現するためにリージョンリーダーをバランスさせるのに対し、`balance-region`はストレージの圧力を再分配し、ストレージスペースの不足などの例外を回避するために各リージョンピアを考慮に入れます。

`balance-leader`と`balance-region`のスケジューリングプロセスは次のようになります：

1. ストアのリソースの可用性に応じてスコアを付けます。
2. `balance-leader`または`balance-region`は、高いスコアを持つストアから低いスコアを持つストアにリーダーやピアを定期的に転送します。

ただし、スコアの計算方法は異なります。`balance-leader`は、ストア内のリーダーに対応するすべてのリージョンサイズの合計を使用しますが、`balance-region`の方法は比較的複雑です。各ノードの特定のストレージ容量に依存する場合、`balance-region`の評価方法は次のようになる場合があります：

- 十分なストレージがある場合は、データの分散を均等にするために使用します。
- ストレージが不足している場合は、ストレージの利用可能性を均等にするために使用します。
- 上記の状況のいずれも該当しない場合は、上記の2つの要素の重み付け合計を使用します。

パフォーマンスが異なるノードが存在するため、異なるストアの負荷分散の重みを設定することもできます。`leader-weight`と`region-weight`は、それぞれリーダーの重みとリージョンの重みを制御するために使用されます（両方のデフォルト値は「1」です）。たとえば、「leader-weight」が「2」に設定されているストアでは、スケジューリングが安定した後、そのノード上のリーダーの数は他のノードの約2倍になります。同様に、「leader-weight」が「0.5」に設定されている場合、そのノード上のリーダーの数は他のノードの約半分になります。

### ホットリージョンのスケジューリング

ホットリージョンのスケジューリングには、`hot-region-scheduler`を使用します。TiDB v3.0以降、プロセスは以下のように実行されます：

1. ストアが報告した情報を基に、ある一定期間にわたって一定の閾値を超える読み込み/書き込みトラフィックを持つホットリージョンをカウントします。

2. これらのリージョンを負荷分散と同様の方法で再分配します。

ホットライトリージョンでは、`hot-region-scheduler`はリージョンピアとリーダーの両方を再分配します。ホットリードリージョンの場合、`hot-region-scheduler`はリーダーの再分配のみ行います。

### クラスターのトポロジー意識

クラスタートポロジー意識により、PDはリージョンのレプリカを可能な限り分散させることができます。これにより、TiKVは高可用性と災害復旧能力を確保します。PDはバックグラウンドですべてのリージョンを定期的にスキャンします。PDは、リージョンの分散が最適ではない場合、ピアの置き換えとリージョンの再分散を行うオペレータを生成します。

リージョンの分布を確認するためのコンポーネントは、`replicaChecker`です。これはスケジューラと似ていますが、無効にすることはできません。`replicaChecker`は、「location-labels」の設定に基づいてスケジュールを行います。たとえば、「[zone,rack,host]」はクラスターの3つのトポロジーレベルを定義します。PDはまずリージョンピアを異なるゾーンにスケジュールし、ゾーンが不足している場合（たとえば、3つのレプリカ用の2つのゾーン）は、異なるラックにスケジュールします。ラックも不足している場合は、さらに異なるホストにスケジュールします。

### スケールダウンと障害復旧

スケールダウンは、コマンドを使用してストアをオフラインにし、「オフライン」とマークするプロセスを指します。PDはスケジューリングによってオフラインノード上のリージョンを他のノードにレプリケートします。障害復旧は、ストアが障害を起こし復旧できない場合に適用されます。この場合、対応するストア上に分散しているピアを持つリージョンは、レプリカを失う可能性があり、PDは他のノードに補充する必要があります。

スケールダウンと障害復旧のプロセスは基本的に同じです。`replicaChecker`は異常な状態のリージョンピアを見つけ、異常なピアを健全なストア上の新しいピアに置き換えるオペレータを生成します。

### リージョンのマージ

リージョンのマージとは、隣接する小さなリージョンを統合するプロセスを指します。これは、データの削除後に大量の小さなリージョンまたは空のリージョンによる不必要なリソース消費を回避するためのものです。リージョンのマージは、`mergeChecker`によって実行され、`replicaChecker`と同様の方法で処理されます。PDはバックグラウンドですべてのリージョンを定期的にスキャンし、連続した小さなリージョンが見つかった場合にオペレータを生成します。

具体的には、新たに分割されたリージョンが[`split-merge-interval`](/pd-configuration-file.md#split-merge-interval)（デフォルトで`1h`）以上存在し、以下の条件が同時に発生する場合、このリージョンはリージョンマージスケジューリングをトリガーします：

- このリージョンのサイズが[`max-merge-region-size`](/pd-configuration-file.md#max-merge-region-size)（デフォルトで20 MiB）よりも小さい

- このリージョンのキーの数が[`max-merge-region-keys`](/pd-configuration-file.md#max-merge-region-keys)（デフォルトで200,000）よりも小さい

## クエリスケジューリングの状態

メトリクス、pd-ctl、ログを通じてスケジューリングシステムの状態を確認することができます。このセクションでは、メトリクスとpd-ctlの方法を簡単に紹介します。詳細については、「[PDモニタリングメトリクス](/grafana-pd-dashboard.md)」および「[PDコントロール](/pd-control.md)」を参照してください。

### オペレータの状態

**Grafana PD/Operator** ページでは、オペレータに関するメトリクスが表示されます。以下のメトリクスが含まれます：

- スケジュールオペレータ作成：オペレータの作成情報
- オペレータの完了時間：各オペレータの実行時間
- オペレータステップの実行時間：オペレータステップの実行時間

次のコマンドを使用して、pd-ctlでオペレータをクエリできます：

- `operator show`：現在のスケジューリングタスクで生成されたすべてのオペレータをクエリします。
- `operator show [admin | leader | region]`：タイプ別にオペレータをクエリします。

### バランスの状態

**Grafana PD/Statistics - Balance** ページでは、負荷分散に関するメトリクスが表示されます。以下のメトリクスが含まれます：

- ストアのリーダー/リージョンスコア：各ストアのスコア
- ストアのリーダー/リージョン数：各ストアのリーダー/リージョンの数
- ストアの利用可能性：各ストアの利用可能なストレージ

pd-ctlのストアコマンドを使用して、各ストアのバランスの状態をクエリできます。

### ホットリージョンの状態

**Grafana PD/Statistics - hotspot** ページでは、ホットリージョンに関するメトリクスが表示されます。以下のメトリクスが含まれます：

- ホットライトリージョンのリーダー/ピア分布：ホットライトリージョンのリーダー/ピアの分布
- ホットリードリージョンのリーダー分布：ホットリードリージョンのリーダーの分布

次のコマンドを使用して、pd-ctlでホットリージョンの状態をクエリできます：

- `hot read`：ホットリードリージョンをクエリします。
- `hot write`：ホットライトリージョンをクエリします。
- `hot store`：ストアごとのホットリージョンの分布をクエリします。
- `region topread [limit]`：読み取りトラフィックの最も多いリージョンをクエリします。
- `region topwrite [limit]`: トップ書き込みトラフィックのリージョンをクエリします

### リージョンの健康状態

**Grafana PD/Cluster/Region health** パネルは、異常な状態にあるリージョンに関するメトリクスを表示します。

pd-ctlを使用して、異常な状態にあるリージョンのリストをクエリすることができます。以下は、pd-ctlで使用できるリージョンチェックコマンドです。

- `region check miss-peer`: ピアが不足しているリージョンをクエリします
- `region check extra-peer`: 追加のピアを持つリージョンをクエリします
- `region check down-peer`: ダウンしたピアを持つリージョンをクエリします
- `region check pending-peer`: 保留中のピアを持つリージョンをクエリします

## スケジューリング戦略の制御

以下の3つの側面からスケジューリング戦略を調整するために、pd-ctlを使用することができます。詳細については、[PD Control](/pd-control.md)を参照してください。

### スケジューラの手動追加/削除

PDは、pd-ctlを通じてスケジューラを直接追加または削除することをサポートしています。例えば：

- `scheduler show`: システムで現在実行中のスケジューラを表示します
- `scheduler remove balance-leader-scheduler`: balance-leader-schedulerを削除（無効化）します
- `scheduler add evict-leader-scheduler 1`: ストア1のすべてのリーダを削除するスケジューラを追加します

### オペレータの手動追加/削除

PDはpd-ctlを通じてオペレータを直接追加または削除することもサポートしています。例えば：

- `operator add add-peer 2 5`: リージョン2にピアをストア5に追加します
- `operator add transfer-leader 2 5`: リージョン2のリーダをストア5に移行します
- `operator add split-region 2`: リージョン2をサイズが均等な2つのリージョンに分割します
- `operator remove 2`: リージョン2の現在保留中のオペレータを削除します

### スケジューリングパラメータの調整

`config show`コマンドを使用してスケジューリング構成を確認し、`config set {key} {value}`コマンドを使用して値を調整することができます。一般的な調整例は以下の通りです：

- `leader-schedule-limit`: リーダスケジューリングの同時実行数を制御します
- `region-schedule-limit`: ピアの追加/削除スケジューリングの同時実行数を制御します
- `enable-replace-offline-replica`: ノードをオフラインにするスケジューリングを有効にするかどうかを制御します
- `enable-location-replacement`: リージョンの分離レベルを処理するスケジューリングを有効にするかどうかを制御します
- `max-snapshot-count`: 各ストアのスナップショットの送受信の最大同時実行数を制御します

## 一般的なシナリオでのPDスケジューリング

このセクションでは、いくつかの典型的なシナリオを通じて、PDのスケジューリング戦略のベストプラクティスを説明します。

### リーダ/リージョンの均等な分布がされていない

PDの評価メカニズムにより、異なるストアのリーダ数およびリージョン数は、負荷分散の状態を十分に反映することができません。そのため、TiKVの実際の負荷やストレージ使用量から負荷の不均等状態を確認する必要があります。

リーダ/リージョンが均等に分布していないことを確認した場合、異なるストアの評価を確認する必要があります。

異なるストアのスコアが近い場合、PDは誤ってリーダ/リージョンが均等に分布していると判断しています。可能な原因は次のとおりです：

- 負荷の不均等を引き起こすホットリージョンが存在するためです。この場合は、[ホットリージョンのスケジューリング](#hot-regions-are-not-evenly-distributed)に基づいてさらに分析する必要があります。
- 多数の空のリージョンや小さいリージョンが存在し、異なるストア間のリーダ数の差が大きく、Raftストアに高い負荷がかかっています。この場合は、[リージョンのマージ](#region-merge-is-slow)のスケジューリングが必要です。
- ストア間でハードウェアおよびソフトウェア環境が異なる場合です。リーダ/リージョンの分布を制御するために、[負荷分散](#load-balancing)を参照し、`leader-weight`および`region-weight`の値を調整することができます。
- その他の不明な理由です。リーダ/リージョンの分布を制御するために、`leader-weight`および`region-weight`の値を調整することもできます。

異なるストアの評価に大きな差がある場合、オペレータに関連するメトリクスを調査し、オペレータの生成と実行に特に注意を払う必要があります。主な状況は以下の2つです：

- オペレータが正常に生成されているが、スケジューリングプロセスが遅い場合、次の可能性があります：

    - デフォルトでは、負荷分散の目的でスケジューリング速度が制限されています。`leader-schedule-limit`または`region-schedule-limit`を大きな値に調整することで、通常のサービスに大きな影響を与えることなく、制約を緩和することができます。また、`max-pending-peer-count`および`max-snapshot-count`で指定された制約も適切に緩和することができます。
    - 他のスケジューリングタスクが同時に実行されており、バランシングが遅くなっている場合、バランシングを他のスケジューリングタスクより優先させる場合は、他のタスクを停止するか、スピードを制限することができます。例えば、バランシングが進行中の場合、一部のノードをオフラインにすると、両方の操作が`region-schedule-limit`のクォータを消費します。この場合、ノードを削除するスケジューラの速度を制限するか、単純に`enable-replace-offline-replica = false`を設定して一時的に無効にすることができます。
    - スケジューリングプロセスが遅すぎる場合。原因を確認するためには、**Operator step duration**メトリクスをチェックすることができます。一般的に、スナップショットの送受信を伴わないステップ（`TransferLeader`、`RemovePeer`、`PromoteLearner`など）はミリ秒で完了するはずですが、スナップショットを含むステップ（`AddLearner`、`AddPeer`など）は数十秒で完了することが期待されます。もし所要時間が明らかに長すぎる場合、TiKVに高い負荷がかかっているか、ネットワークにボトルネックがある可能性があり、具体的な分析が必要です。

- PDが対応するバランシングスケジューラを生成できない場合、次の可能性があります：

    - スケジューラがアクティブ化されていない。例えば、対応するスケジューラが削除されたか、その制限が「0」に設定されている場合。
    - その他の制約。例えば、システム内の`evict-leader-scheduler`が対応するストアへのリーダの移行を阻止している。または、ラベルプロパティが設定されており、一部のストアがリーダを拒否します。
    - クラスタトポロジーからの制約。例えば、3つのデータセンターにわたる3つのレプリカからなるクラスタでは、各リージョンの3つのレプリカは、レプリカの隔離のために異なるデータセンターに分散して配置されます。もし、これらのデータセンター間でストア数が異なる場合、スケジューリングは各データセンター内でバランスが取れた状態にしかならず、グローバルにバランスが取れる状態にはなりません。

### ノードのオフライン化が遅い

このシナリオでは、関連するメトリクスを使用してオペレータの生成と実行を調査する必要があります。

オペレータが正常に生成されたが、スケジューリングプロセスが遅い場合、次の可能性があります：

- デフォルトでは、スケジューリング速度が制限されています。`leader-schedule-limit`または`replica-schedule-limit`を大きな値に調整することができます。同様に、`max-pending-peer-count`および`max-snapshot-count`で指定された制約も緩和することができます。
- 他のスケジューリングタスクが同時に実行され、システムのリソースを競合している場合、[リーダ/リージョンの均等な分布がされていない](#leadersregions-are-not-evenly-distributed)の解決策を参照してください。
- 単一のノードをオフラインにすると、削除するノード上に分散しているリージョンのリーダ（3レプリカの設定ではおおよそ1/3）の数が処理されるため、この単一のノードによってスナップショットが生成される速度に制限がかかります。リーダの移行を行うためには、手動で`evict-leader-scheduler`を追加してリーダを移行させることで速度を速めることができます。

対応するオペレータが生成されない場合、次の可能性があります：

- オペレータが停止されているか、`replica-schedule-limit`が「0」に設定されています。
- リージョンのマイグレーションに適したノードが存在しない。例えば、同じラベルの代替ノードの利用可能容量サイズが20％未満の場合、PDはストレージが不足することを避けるため、スケジューリングを停止します。この場合、ノードを追加するか、一部のデータを削除してスペースを解放する必要があります。

### ノードのオンライン化が遅い

現在、ノードのオンライン化はバランスリージョンのメカニズムを通じてスケジュールされています。トラブルシューティングについては、[リーダ/リージョンの均等な分布がされていない](#leadersregions-are-not-evenly-distributed)をご参照ください。

### ホットリージョンが均等に分布していない

ホットリージョンのスケジューリングの問題は、主に以下のカテゴリに分類されます。

- PDのメトリクスを通じてホットリージョンが観測されるが、スケジューリングの速度がホットリージョンの再分配に追いつかない場合。

    **解決策**：`hot-region-schedule-limit`を大きな値に調整し、他のスケジューラの制約クォータを減らしてホットリージョンのスケジューリングを高速化するか、`hot-region-cache-hits-threshold`を小さな値に調整してPDをトラフィックの変化により敏感にさせることができます。

- 単一のリージョンにホットスポットが形成されている。例えば、小さなテーブルが大量のリクエストによって集中的にスキャンされる場合です。これもPDのメトリクスから検出することができます。単一のホットスポットは実際には分散することができないため、このようなリージョンを分割するために`split-region`オペレータを手動で追加する必要があります。

- 一部のノードの負荷がTiKV関連のメトリクスから他のノードよりも明らかに高い場合、それがシステム全体のボトルネックになります。現在、PDはトラフィック分析のみによりホットスポットをカウントしていますので、特定のシナリオではホットスポットを検出できない可能性があります。たとえば、一部のリージョンに対して集中的なポイントルックアップリクエストがある場合、トラフィックで検出することは明らかではありませんが、高いQPSはキーモジュールのボトルネックを引き起こす可能性があります。

    **解決策**：まず、特定のビジネスに基づいてホットリージョンが形成されるテーブルを特定します。その後、このテーブルのすべてのリージョンを均等に分散させるために`scatter-range-scheduler`スケジューラを追加します。また、TiDBは、この操作を簡素化するためのHTTP APIでインターフェースを提供しています。詳細については、[TiDB HTTP API](https://github.com/pingcap/tidb/blob/master/docs/tidb_http_api.md)を参照してください。

### リージョンのマージが遅い
スロースケジューリングと同様に、リージョンマージの速度はおそらく`merge-schedule-limit`と`region-schedule-limit`の設定によって制限されているか、リージョンマージスケジューラが他のスケジューラと競合している可能性が高いです。具体的なソリューションは次のとおりです。

- システムのメトリクスから、空のリージョンが大量に存在することがわかっている場合、`max-merge-region-size`と`max-merge-region-keys`を小さな値に調整してマージを高速化することができます。なぜなら、マージプロセスにはレプリカの移行が関与するため、マージするリージョンが小さいほど、マージが速くなります。もしマージオペレータが既に迅速に生成されている場合、プロセスをさらに高速化するためには、`patrol-region-interval`を`10ms`に設定することができます（この構成項目のデフォルト値はTiDBのv5.3.0以降では`10ms`です）。これにより、リージョンのスキャンが速くなりますが、CPUの消費が増加します。

- 多くのテーブルが作成され、その後に空になっています（切り捨てられたテーブルを含む）。これらの空のリージョンは、分割テーブル属性が有効になっている場合にはマージすることができません。これを無効にするには、次のパラメータを調整します。

    - TiKV: `split-region-on-table`を`false`に設定します。このパラメータは動的に変更できません。
    - PD: PDコントロールを使用して、クラスタの状況に必要なパラメータを設定します。

        - クラスタにTiDBインスタンスがなく、[`key-type`](/pd-control.md#config-show--set-option-value--placement-rules)の値が`raw`または`txn`に設定されているとします。この場合、PDは`enable-cross-table-merge`の値に関係なく、テーブルをまたいでリージョンをマージすることができます。`key-type`パラメータを動的に変更できます。

        {{< copyable "shell-regular" >}}

        ```bash
        config set key-type txn
        ```

        - クラスタにTiDBインスタンスがあり、`key-type`の値が`table`に設定されているとします。この場合、PDは`enable-cross-table-merge`の値が`true`に設定されている場合にのみ、テーブルをまたいでリージョンをマージすることができます。`key-type`パラメータを動的に変更できます。

        {{< copyable "shell-regular" >}}

        ```bash
        config set enable-cross-table-merge true
        ```

        変更が反映されない場合は、[FAQ - TiKV/PDの変更した`toml`設定が反映されない理由](/faq/deploy-and-maintain-faq.md#why-the-modified-toml-configuration-for-tikvpd-does-not-take-effect)を参照してください。

        > **注意：**
        >
        > Placement Rulesを有効にした後は、デコードの失敗を避けるために`key-type`の値を適切に切り替えてください。

v3.0.4およびv2.1.16またはそれ以前のバージョンでは、特定の状況下で（主にテーブルを削除した後に）リージョンの`approximate_keys`が不正確であり、このため`max-merge-region-keys`の制約を破るキーの数が発生します。この問題を回避するために、`max-merge-region-keys`を大きな値に調整することができます。

### TiKVノードのトラブルシューティング

TiKVノードが失敗した場合、PDはその対応するノードを30分後に**down**状態に設定し（構成項目`max-store-down-time`でカスタマイズ可能）、関連するリージョンのレプリカのリバランスを行います。

実際には、ノードの障害が回復不能な場合はすぐにオフラインにすることができます。これにより、PDは他のノードで迅速にレプリカを補充し、データ損失のリスクを減らすことができます。一方、ノードが回復可能だが回復が30分で完了できない場合は、一時的に`max-store-down-time`を大きな値に調整して、タイムアウト後に不要なレプリカの補充とリソースの無駄遣いを避けることができます。

TiDB v5.2.0では、TiKVは遅いTiKVノードの検出メカニズムを導入しています。このメカニズムはTiKVでリクエストをサンプリングし、その結果1から100までのスコアを算出します。スコアが80以上のTiKVノードは遅いとマークされます。[`evict-slow-store-scheduler`](/pd-control.md#scheduler-show--add--remove--pause--resume--config--describe)を追加して遅いノードを検出し、スケジュールすることができます。1つのTiKVノードが遅いと検出され、遅延スコアがデフォルトでリミット（80）に達した場合、このノードのLeaderは（`evict-leader-scheduler`と同様に）追放されます。

> **注意：**
>
> **Leaderの追放**は、PDがTiKV遅いノードにスケジューリングリクエストを送信し、その後TiKVが受信したスケジューリングリクエストを順次実行することで実現されます。遅いI/Oなどの要因により、遅いノードにはリクエストが蓄積される可能性があり、遅いI/Oなどの要因により、遅いノードにはリクエストが蓄積される可能性があり（Leaderの追放）のリクエスト処理が遅れる場合があります。そのため、`evict-slow-store-scheduler`を有効にする場合は、この状況を緩和するために[`store-io-pool-size`](/tikv-configuration-file.md#store-io-pool-size-new-in-v530)も有効にすることをお勧めします。