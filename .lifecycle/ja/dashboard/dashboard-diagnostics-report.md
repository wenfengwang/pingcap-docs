---
title: TiDB ダッシュボード診断レポート
summary: TiDB ダッシュボード診断レポートについて学びます。
aliases: ['/docs/dev/dashboard/dashboard-diagnostics-report/']
---

# TiDB ダッシュボード診断レポート

このドキュメントでは、診断レポートの内容と表示のヒントについて紹介します。クラスター診断ページにアクセスしてレポートを生成するには、[TiDB ダッシュボード クラスター診断ページ](/dashboard/dashboard-diagnostics-access.md)を参照してください。

## レポートの表示

診断レポートは以下の部分から構成されています。

* 基本情報: 診断レポートの時間範囲、クラスターのハードウェア情報、クラスタートポロジのバージョン情報が含まれます。
* 診断情報: 自動診断の結果が表示されます。
* 負荷情報: サーバー、TiDB、PD、または TiKV の CPU、メモリなどの負荷情報が含まれます。
* 概要情報: 各 TiDB、PD、または TiKV モジュールの消費時間とエラー情報が含まれます。
* TiDB/PD/TiKV 監視情報: 各コンポーネントの監視情報が含まれます。
* 構成情報: 各コンポーネントの構成情報が含まれます。

診断レポートの例は次のとおりです。

![サンプルレポート](/media/dashboard/dashboard-diagnostics-example-table.png)

上記の画像では、青い上部のボックスにある **Total Time Consume** が報告名です。赤いボックスの情報は、このレポートの内容とレポート内の各フィールドの意味を説明しています。

このレポートでは、以下のようにいくつかの小さなボタンが説明されています。

* **i** アイコン: **i** アイコンにマウスを移動すると、その行の説明が表示されます。
* **expand**: **expand** をクリックすると、この監視メトリックの詳細を表示できます。たとえば、上記の画像の `tidb_get_token` の詳細情報には、各 TiDB インスタンスの待ち時間の監視情報が含まれます。
* **collapse**: **expand** とは逆に、詳細な監視情報を折りたたむためのボタンです。

すべての監視メトリックは基本的に TiDB Grafana 監視ダッシュボードのものに対応しています。モジュールが異常であると判断された場合は、TiDB Grafana でより多くの監視情報を表示できます。

また、このレポートにおける `TOTAL_TIME` および `TOTAL_COUNT` のメトリックは Prometheus から読み取った監視データですので、その統計には計算の不正確さが存在する場合があります。

このレポートの各部分は以下のように紹介されます。

### 基本情報

#### 診断時間範囲

診断レポートの生成に含まれる時間範囲には開始時刻と終了時刻が含まれます。

![レポート時間範囲](/media/dashboard/dashboard-diagnostics-report-time-range.png)

#### クラスターハードウェア情報

クラスターハードウェア情報には、クラスター内の各サーバーの CPU、メモリ、ディスクなどの情報が含まれます。

![クラスターハードウェアレポート](/media/dashboard/dashboard-diagnostics-cluster-hardware.png)

上記の表のフィールドは次のように説明されます:

* `HOST`: サーバーの IP アドレス。
* `INSTANCE`: サーバーに展開されたインスタンスの数。例えば、`pd * 1` はこのサーバーに 1 つの PD インスタンスが展開されていることを意味し、`tidb * 2 pd * 1` はこのサーバーに 2 つの TiDB インスタンスと 1 つの PD インスタンスが展開されていることを意味します。
* `CPU_CORES`: サーバーの CPU コア数 (物理コアまたは論理コア) を示します。
* `MEMORY`: サーバーのメモリサイズを示します。単位は GB です。
* `DISK`: サーバーのディスクサイズを示します。単位は GB です。
* `UPTIME`: サーバーの起動時間。単位は日です。

#### クラスタートポロジ情報

`Cluster Info` テーブルにはクラスタートポロジ情報が表示されます。この表の情報は、TiDB の [information_schema.cluster_info](/information-schema/information-schema-cluster-info.md) システムテーブルから取得されたものです。

![クラスター情報](/media/dashboard/dashboard-diagnostics-cluster-info.png)

上記の表のフィールドは次のように説明されます:

* `TYPE`: ノードのタイプ。
* `INSTANCE`: インスタンスのアドレス(es) で、`IP:PORT` 形式の文字列です。
* `STATUS_ADDRESS`: HTTP API サービスアドレス。
* `VERSION`: 対応ノードのセマンティックバージョン番号。
* `GIT_HASH`: ノードバージョンをコンパイルするときの Git コミットハッシュであり、2 つのノードが完全に一致するバージョンであるかどうかを識別するために使用されます。
* `START_TIME`: 対応ノードの開始時間。
* `UPTIME`: 対応ノードの起動時間。

### 診断情報

TiDB には組み込みの自動診断結果があります。各フィールドの説明については、[information_schema.inspection-result](/information-schema/information-schema-inspection-result.md) システムテーブルを参照してください。

### 負荷情報

#### ノード負荷情報

`Node Load Info` テーブルには、サーバーノードの負荷情報が表示されます。指定した時間範囲内でのサーバーの以下のメトリックの平均値 (AVG)、最大値 (MAX)、最小値 (MIN) が含まれます:

* CPU 使用率 (最大値は `100%`)
* メモリ使用率
* ディスク I/O 使用率
* ディスク書き込み遅延
* ディスク読み取り遅延
* 1 秒あたりのディスク読み込みバイト数
* 1 秒あたりのディスク書き込みバイト数
* ノードネットワークによって受信されたバイト数
* ノードネットワークから送信されたバイト数
* ノードで使用されている TCP 接続数
* ノードのすべての TCP 接続数

![サーバー負荷情報レポート](/media/dashboard/dashboard-diagnostics-node-load-info.png)

#### インスタンス CPU 使用率

`Instance CPU Usage` テーブルには、各 TiDB/PD/TiKV プロセスの CPU 使用率の平均値 (AVG)、最大値 (MAX)、最小値 (MIN) が表示されます。プロセスの最大 CPU 使用率は `100% * CPU 論理コア数` です。

![インスタンス CPU 使用率レポート](/media/dashboard/dashboard-diagnostics-process-cpu-usage.png)

#### インスタンスメモリ使用量

`Instance Memory Usage` テーブルには、各 TiDB/PD/TiKV プロセスが占有するメモリバイト数の平均値 (AVG)、最大値 (MAX)、最小値 (MIN) が表示されます。

![インスタンスメモリ使用量レポート](/media/dashboard/dashboard-diagnostics-process-memory-usage.png)

#### TiKV スレッド CPU 使用率

`TiKV Thread CPU Usage` テーブルには、各モジュールスレッドの CPU 使用率の平均値 (AVG)、最大値 (MAX)、最小値 (MIN) が表示されます。プロセスの最大 CPU 使用率は、対応する構成のスレッド数を使用します。

![TiKV スレッド CPU 使用率レポート](/media/dashboard/dashboard-diagnostics-thread-cpu-usage.png)

上記のテーブルでは、

* `CONFIG_KEY`: 対応するモジュールのスレッド設定。
* `CURRENT_CONFIG_VALUE`: レポート生成時の構成値。

> **注記:**
>
> `CURRENT_CONFIG_VALUE` はレポート生成時の値です。現在、一部の過去の時点での構成値は取得できません。

#### `TiDB/PD Goroutines Count`

`TiDB/PD Goroutines Count` テーブルには、TiDB または PD のゴルーチン数の平均値 (AVG)、最大値 (MAX)、最小値 (MIN) が表示されます。ゴルーチン数が 2,000 を超えると、プロセスの並行性が高すぎて全体のリクエスト待ち時間に影響を与えます。

![TiDB/PD ゴルーチン数レポート](/media/dashboard/dashboard-diagnostics-goroutines-count.png)

### 概要情報

#### 各コンポーネントの消費時間

`Time Consumed by Each Component` テーブルには、クラスターの TiDB、PD、TiKV モジュールの監視された消費時間と時間比率が表示されます。デフォルトの時間単位は秒です。このテーブルを使用して、どのモジュールがより多く時間を消費しているかを迅速に特定できます。

![時間消費レポート](/media/dashboard/dashboard-diagnostics-total-time-consume.png)

上記のテーブルの列のフィールドは次のように説明されます:

* `METRIC_NAME`: 監視メトリックの名前。
* `Label`: 監視メトリックのラベル情報。各ラベルの詳細な監視情報を表示するには **expand** をクリックしてください。
* `TIME_RATIO`: この監視メトリックで消費された総時間の `TIME_RATIO` が `1` の監視行の総時間に対する比率。たとえば、`kv_request` の総消費時間は `1.65` (`38325.58`/`23223.86`) 倍の `tidb_query` の消費時間です。KV リクエストは同時に実行されるため、すべての KV リクエストの総時間は総クエリ (`tidb_query`) 実行時間を超える場合があります。
* `TOTAL_TIME`: この監視メトリックで消費された総時間。
* `TOTAL_COUNT`: この監視メトリックが実行された総数。
* `P999`: この監視メトリックの最大 P999 時間。
* `P99`: この監視メトリックの最大 P99 時間。
* `P90`: この監視メトリックの最大 P90 時間。
* `P80`: この監視メトリックの最大 P80 時間。

以下の画像は、上記の監視メトリックで関連モジュールの時間消費の関係を示しています。

![各モジュールの時間消費関係](/media/dashboard/dashboard-diagnostics-time-relation.png)

上の画像では、黄色のボックスは TiDB 関連の監視メトリックです。青色のボックスは TiKV 関連の監視メトリックであり、灰色のボックスは一時的に特定の監視メトリックに対応していません。

上の画像では、`tidb_query` の時間消費には次の 4 つの部分が含まれます:

* `get_token`
* `parse`
* `compile`
* `execute`

`execute` の時間には次の部分が含まれます:

* `wait_start_tso`
* 現在、TiDB レイヤで実行されている処理時間は監視されていません
* KVのリクエスト時間
* `KV_backoff`時間、KVリクエストが失敗した後のバックオフ時間

上記の中で、KVリクエスト時間には以下の部分が含まれます:

* リクエストのネットワーク送信と受信によって消費される時間。現在、この項目に関するモニタリングメトリックはありません。おおよその見積もりをするために、KVリクエスト時間から`tikv_grpc_message`の時間を引くことができます。
* `tikv_grpc_message`の時間消費。

上記の中で、`tikv_grpc_message`の時間消費には以下の部分が含まれます:

* コプロセッサリクエストの時間消費。これはCOPタイプのリクエストを処理することを指します。この時間消費には以下の部分が含まれます:
    * `tikv_cop_wait`: リクエストキューによって消費される時間。
    * `Coprocessor handle`: コプロセッサリクエストの処理にかかる時間。

* `tikv_scheduler_command`の時間消費には以下の部分が含まれます:
    * `tikv_scheduler_processing_read`: 読み込みリクエストの処理にかかる時間。
    * `tikv_storage_async_request`でスナップショットを取得するために消費される時間（スナップショットはこのモニタリングメトリックのラベルです）。
    * 書き込みリクエストの処理にかかる時間。この時間消費には以下の部分が含まれます:
        * `tikv_scheduler_latch_wait`: ラッチを待つために消費される時間。
        * `tikv_storage_async_request`での書き込みの時間消費（このモニタリングメトリックのラベルは"write"です）。

上記のメトリックの中で、`tikv_storage_async_request`での書き込みの時間消費はRaft KVsの書き込みにかかる時間で、以下の部分を含みます:

* `tikv_raft_propose_wait`
* `tikv_raft_process`は主に`tikv_raft_append_log`を含みます
* `tikv_raft_commit_log`
* `tikv_raft_apply_wait`
* `tikv_raft_apply_log`

上記の時間消費の関係に基づいて、`TOTAL_TIME`、P999時間、およびP99時間を使用して、どのモジュールがより多くの時間を消費しているかを決定し、それから関連するモニタリングメトリックを見ることができます。

> **注意:**
>
> Raft KVs書き込みは1つのバッチで処理されるかもしれませんので、各モジュールの時間消費を測定するために`TOTAL_TIME`を使用することは、Raft KV書き込みに関連するモニタリングメトリックには適用されません。特に、`tikv_raft_process`、`tikv_raft_append_log`、`tikv_raft_commit_log`、`tikv_raft_apply_wait`、`tikv_raft_apply_log`には適用されません。この状況では、各モジュールの時間消費をP999とP99の時間と比較することがより合理的です。
>
> その理由は、例えば10の非同期書き込みリクエストがある場合、Raft KVsは内部的に10のリクエストを1つのバッチ実行にパックし、実行時間は1秒です。したがって、各リクエストの実行時間は1秒であり、10のリクエストの合計時間は10秒ですが、Raft KV処理の合計時間は1秒です。`TOTAL_TIME`を使用して時間消費を測定すると、残りの9秒がどこに消費されたのかが理解できません。Raft KVのモニタリングメトリックと他の前のモニタリングメトリックとの違いは、リクエストの合計数（`TOTAL_COUNT`）からもわかります。

#### 各コンポーネントで発生したエラー

`各コンポーネントで発生したエラー` テーブルには、TiDBとTiKVで発生したエラーの総数が表示されます。例: バイナリログの書き込みの失敗、`tikv server is busy`、`TiKV channel full`、`tikv write stall`。各エラーの具体的な意味については、その行のコメントを確認できます。

![各コンポーネントで発生したエラーレポート](/media/dashboard/dashboard-diagnostics-error.png)

#### TiDB/PD/TiKVの特定のモニタリング情報

この部分には、TiDB、PD、またはTiKVのより具体的なモニタリング情報が含まれます。

#### TiDB関連のモニタリング情報

##### TiDBコンポーネントによって消費される時間

このテーブルには、それぞれのTiDBモジュールによって消費される時間とその消費時間割合が表示されます。これは概要の`time consume`テーブルと類似していますが、このテーブルのラベル情報はより詳細です。

##### TiDBサーバー接続

このテーブルには、各TiDBインスタンスのクライアント接続数が表示されます。

##### TiDBトランザクション

このテーブルには、トランザクションに関連するモニタリングメトリックが表示されます。

![トランザクションレポート](/media/dashboard/dashboard-diagnostics-tidb-txn.png)

* `TOTAL_VALUE`: レポートの時間範囲内でのすべての値（SUM）の合計。
* `TOTAL_COUNT`: このモニタリングメトリックの合計発生数。
* `P999`: このモニタリングメトリックの最大P999値。
* `P99`: このモニタリングメトリックの最大P99値。
* `P90`: このモニタリングメトリックの最大P90値。
* `P80`: このモニタリングメトリックの最大P80値。

例:

上記のテーブルでは、レポートの時間範囲内で`tidb_txn_kv_write_size`に関して、約181,296のKV書き込みトランザクションがあり、合計KV書き込みサイズは266.772 MBであり、そのうち単一のKV書き込みトランザクションの最大P999、P99、P90、P80値はそれぞれ116.913 KB、1.996 KB、1.905 KB、および1.805 KBです。

##### DDLオーナー

![TiDB DDLオーナーレポート](/media/dashboard/dashboard-diagnostics-tidb-ddl.png)

上記のテーブルは、`2020-05-21 14:40:00`以降、クラスターの`DDL OWNER`が`10.0.1.13:10080`ノードにあることを示しています。オーナーが変更される場合、表の上に複数行のデータが存在し、`Min_Time`列は対応する既知のオーナーの最小時間を示します。

> **注意:**
>
> オーナー情報が空の場合、これはその期間にオーナーが存在しないことを意味するわけではありません。なぜなら、この場合、DDLオーナーは`ddl_worker`のモニタリング情報に基づいて決定されるため、この期間に`ddl_worker`がDDLジョブを行っていない可能性があるため、オーナー情報が空になります。

TiDBの他のモニタリングテーブルは以下の通りです:

* 統計情報: TiDB統計情報に関連するモニタリングメトリックが表示されます。
* Top 10遅いクエリ: レポートの時間範囲内のTop 10遅いクエリ情報が表示されます。
* Top 10クエリダイジェスト別グループ: レポート時間範囲内のTop 10遅いクエリ情報が、SQLのフィンガープリントに基づいて集約されて表示されます。
* 異なる実行プランの遅いクエリ: レポートの時間範囲内で実行プランが変化したSQLステートメントが表示されます。

#### PD関連モニタリング情報

PDモジュールのモニタリング情報に関連するテーブルは次の通りです:

* `PDコンポーネントによって消費される時間`: PDの関連モジュールのモニタリングメトリックによって消費される時間。
* `Leader/Regionのバランス`: レポートの時間範囲内でクラスターで発生した`balance-region`や`balance leader`のモニタリング情報。たとえば、`tikv_note_1`からスケジュールアウトされたリーダーの数や、スケジュールインされたリーダーの数などが含まれます。
* `クラスターステータス`: 総TiKVノード数、総クラスター保存容量、リージョンの数、オフラインTiKVノードの数などのクラスターステータス情報。
* `ストアステータス`: 各TiKVノードのステータス情報。リージョンスコア、リーダースコア、リージョン/リーダーの数などが含まれます。
* `Etcdのステータス`: PDにおけるetcdに関する情報。

#### TiKV関連のモニタリング情報

TiKVモジュールのモニタリング情報に関連するテーブルは次の通りです:

* `TiKVコンポーネントによって消費される時間`: TiKVの関連モジュールによって消費される時間。
* `RocksDBによって消費される時間`: TiKVにおけるRocksDBによって消費される時間。
* `TiKVエラー`: TiKVの各モジュールに関連するエラー情報。
* `TiKVエンジンサイズ`: TiKVの各ノードにおける列ファミリーの格納データのサイズ。
* `Coprocessor情報`: TiKVのCoprocessorモジュールに関連するモニタリング情報。
* `Raft情報`: TiKVにおけるRaftモジュールのモニタリング情報。
* `スナップショット情報`: TiKVにおけるスナップショットに関連するモニタリング情報。
* `GC情報`: TiKVにおけるガベージコレクション（GC）に関連するモニタリング情報。
* `キャッシュヒット`: TiKVにおけるRocksDBの各キャッシュのヒット率情報。

### 設定情報

設定情報には、レポートの時間範囲内でいくつかのモジュールの設定値が表示されます。ただし、他のいくつかのモジュールの設定の履歴値は取得できないため、これらの設定値の表示は現在の（レポートが生成された時点の）値です。

レポートの時間範囲内で、以下のテーブルには、設定値が報告開始時の値として表示されている項目が含まれています:

* `スケジューラの初期設定`: レポート開始時のPDのスケジューリング関連設定の初期値。
* `TiDB GCの初期設定`: レポート開始時のTiDB GC関連設定の初期値。
* `TiKV RocksDBの初期設定`: レポート開始時のTiKV RocksDB関連設定の初期値。
* `TiKV RaftStoreの初期設定`: レポート開始時のTiKV RaftStore関連設定の初期値。

レポートの時間範囲内で、設定が変更された場合、以下のテーブルには変更された設定の記録が含まれています:

* `スケジューラの設定変更履歴`
* `TiDB GCの設定変更履歴`
* `TiKV RocksDB 設定変更履歴`
* `TiKV RaftStore 設定変更履歴`

例：

![スケジューラ設定変更履歴レポート](/media/dashboard/dashboard-diagnostics-config-change.png)

上記の表は、報告期間内で `leader-schedule-limit` 設定パラメータが変更されたことを示しています：

* `2020-05-22T20:00:00+08:00`： 報告開始時点では、`leader-schedule-limit` の設定値は `4` であり、これは設定が変更されたことを意味するのではなく、報告期間内の開始時点での設定値が `4` であることを示しています。
* `2020-05-22T20:07:00+08:00`： `leader-schedule-limit` の設定値が `8` であり、これはこの設定値が `2020-05-22T20:07:00+08:00` 前後に変更されたことを示しています。

以下の表は、報告生成時点での TiDB、PD、および TiKV の現在の設定を示しています：

* `TiDBの現在の設定`
* `PDの現在の設定`
* `TiKVの現在の設定`

## 比較レポート

2つの時間範囲の比較レポートを生成できます。レポートの内容は単一の時間範囲用のレポートと同じですが、比較列が追加され、2つの時間範囲の違いが示されます。以下のセクションでは、比較レポートのユニークな表と比較レポートの表示方法について説明します。

まず、基本情報の `Compare Report Time Range` レポートには、比較のための2つの時間範囲が表示されます：

![比較レポート時間範囲レポート](/media/dashboard/dashboard-diagnostics-compare-time.png)

上記の表では、`t1` は通常の時間範囲、または基準の時間範囲です。`t2` は異常の時間範囲です。

遅いクエリに関連する表は以下のように表示されます：

* `時間範囲t2内の遅いクエリ`： `t1` には存在せず、`t2` 内にのみ現れる遅いクエリを示します。
* `時間範囲t1内のトップ10遅いクエリ`： `t1` 中のトップ10の遅いクエリです。
* `時間範囲t2内のトップ10遅いクエリ`： `t2` 中のトップ10の遅いクエリです。

### DIFF_RATIO の説明

このセクションでは、`Instance CPU 使用率` テーブルを使用して `DIFF_RATIO` を説明します。

![インスタンス CPU 使用率比較レポート](/media/dashboard/dashboard-diagnostics-compare-instance-cpu-usage.png)

* `t1.AVG`、`t1.MAX`、`t1.Min` は、`t1` 中の CPU 使用率の平均値、最大値、最小値です。
* `t2.AVG`、`t2.MAX`、`t2.Min` は、`t2` 中の CPU 使用率の平均値、最大値、最小値です。
* `AVG_DIFF_RATIO` は `t1` と `t2` の間の平均値の `DIFF_RATIO` です。
* `MAX_DIFF_RATIO` は `t1` と `t2` の間の最大値の `DIFF_RATIO` です。
* `MIN_DIFF_RATIO` は `t1` と `t2` の間の最小値の `DIFF_RATIO` です。

`DIFF_RATIO`：2つの時間範囲の間の違いを示します。次の値があります：

* モニタリングメトリクスが `t2` 内でのみ値を持ち、`t1` 内には値を持たない場合、`DIFF_RATIO` の値は `1` です。
* モニタリングメトリクスが `t1` 内でのみ値を持ち、`t2` 時間範囲内には値を持たない場合、`DIFF_RATIO` の値は `-1` です。
* `t2` の値が `t1` よりも大きい場合、`DIFF_RATIO` = `(t2の値 / t1の値)-1`
* `t2` の値が `t1` よりも小さい場合、`DIFF_RATIO` = `1-(t1の値 / t2の値)`

たとえば、上記の表では、`t2` 内での `tidb` ノードの平均CPU使用率は、`t1` 内よりも2.02倍高く、つまり `2.02` = `1240/410-1` です。

### 最大の異なるアイテムテーブル

`最大の異なるアイテム` テーブルは、2つの時間範囲のモニタリングメトリクスを比較し、それらの違いに従って並べ替えます。このテーブルを使用すると、2つの時間範囲で一番違いの大きいモニタリングメトリクスをすばやく特定できます。以下は例です：

![最大の異なるアイテムテーブル](/media/dashboard/dashboard-diagnostics-maximum-different-item.png)

* `Table`：このモニタリングメトリクスが比較レポート内のどのテーブルから出てきたかを示します。たとえば、`TiKV, coprocessor_info` は TiKV コンポーネントの `coprocessor_info` テーブルから来たことを示します。
* `METRIC_NAME`：モニタリングメトリクスの名前です。`expand` をクリックして、メトリクスの異なるラベルの比較を表示できます。
* `LABEL`：モニタリングメトリクスに対応するラベルです。たとえば、`TiKV Coprocessor scan` のモニタリングメトリクスには `instance` 、 `req` 、 `tag` 、 `sql_type` の4つのラベルがあります。これらは、TiKVのアドレス、リクエストタイプ、操作タイプ、および操作列ファミリです。
* `MAX_DIFF`：`DIFF_RATIO` の計算による違いの値。 `t1.VALUE` と `t2.VALUE` の `DIFF_RATIO` の計算結果です。

上記のテーブルから、`t2` 時間範囲での Coprocessor のリクエストが `t1` 時間範囲よりも数多くあり、`t2` における TiDB の SQL 解析時間がはるかに長いことがわかります。