---
title: TiDBログのバックアップとPITRのアーキテクチャ
summary: TiDBログのバックアップとポイントインタイムリカバリ（PITR）のアーキテクチャについて学ぶ
---

# TiDBログのバックアップとPITRのアーキテクチャ

本文書では、Backup & Restore（BR）ツールを使用したTiDBログのバックアップとポイントインタイムリカバリ（PITR）のアーキテクチャとプロセスについて紹介します。

## アーキテクチャ

ログのバックアップとPITRのアーキテクチャは以下の通りです:

![BRログのバックアップとPITRアーキテクチャ](/media/br/br-log-arch.png)

## ログバックアップのプロセス

クラスターログのバックアッププロセスは以下の通りです:

![BRログのバックアッププロセスデザイン](/media/br/br-log-backup-ts.png)

ログバックアッププロセスに関与するシステムコンポーネントと主要な概念:

* **ローカルメタデータ**: 個々のTiKVノードによってバックアップされたメタデータを示し、ローカルチェックポイントTS、グローバルチェックポイントTS、およびバックアップファイル情報を含みます。
* **ローカルチェックポイントTS**（ローカルメタデータ内）: このTiKVノードで生成されたすべてのログが、指定されたストレージにバックアップされたローカルチェックポイントTSより前に生成されたことを示します。
* **グローバルチェックポイントTS**: すべてのTiKVノードで生成されたログが、指定のストレージにバックアップされたグローバルチェックポイントTSより前に生成されたことを示します。TiDB Coordinatorは、これをすべてのTiKVノードのローカルチェックポイントTSを収集し、PDに報告することで計算します。
* **TiDB Coordinator**: Coordinatorとして選出されたTiDBノードは、全体のログバックアップタスクの進捗（グローバルチェックポイントTS）を収集および計算する責任を持ちます。このコンポーネントは設計上状態を持たず、その障害時には生き残ったTiDBノードから新しいCoordinatorが選出されます。
* **TiKVログバックアップオブザーバ**: TiDBクラスター内の各TiKVノードで実行され、ログデータのバックアップを担当します。TiKVノードが障害が発生した場合は、リージョン再選出後にそのデータ範囲のバックアップを他のTiKVノードが引き継ぎ、それらのノードはグローバルチェックポイントTSから失敗した範囲のデータのバックアップを開始します。

完全なバックアッププロセスは以下の通りです:

1. BRは `br log start` コマンドを受け取ります。

   * BRはチェックポイントTS（ログバックアップの開始時刻）およびバックアップタスクの保存先パスを解析します。
   * **ログバックアップタスクの登録**: BRはPDにログバックアップタスクを登録します。

2. TiKVはログバックアップタスクの作成と更新を監視します。

   * **ログバックアップタスクの取得**: 各TiKVノードのログバックアップオブザーバはPDからログバックアップタスクを取得し、指定された範囲内のログデータをバックアップします。

3. ログバックアップオブザーバは連続的にKV変更ログをバックアップします。

   * **KV変更データの読み取り**: KV変更データを読み取り、[バックアップファイル内のカスタム形式](#log-backup-files)に変更ログを保存します。
   * **グローバルチェックポイントTSの取得**: PDからグローバルチェックポイントTSを取得します。
   * **ローカルメタデータの生成**: ローカルチェックポイントTS、グローバルチェックポイントTS、およびバックアップファイル情報を含むバックアップタスクのローカルメタデータを生成します。
   * **ログデータおよびメタデータのアップロード**: バックアップファイルおよびローカルメタデータを定期的に保存先ストレージにアップロードします。
   * **GCの設定**: PDに対して、[TiDBのGCメカニズム](/garbage-collection-overview.md)によってまだバックアップされていないデータ（ローカルチェックポイントTSより大きいもの）が削除されないように要求を行います。

4. TiDB Coordinatorはログバックアップタスクの進捗を監視します。

   * **バックアップの進捗を監視**: すべてのTiKVノードをポーリングして各リージョンのバックアップ進捗（リージョンチェックポイントTS）を取得します。
   * **グローバルチェックポイントTSの報告**: リージョンチェックポイントTSに基づいて、全体のログバックアップタスクの進捗（グローバルチェックポイントTS）を計算し、PDにグローバルチェックポイントTSを報告します。

5. PDはログバックアップタスクの状態を永続化し、`br log status`を使用して表示することができます。

## PITRのプロセス

PITRのプロセスは以下の通りです:

![ポイントインタイムリカバリプロセスデザイン](/media/br/pitr-ts.png)

完全なPITRプロセスは以下の通りです:

1. BRは `br restore point` コマンドを受け取ります。

   * BRは完全バックアップデータのアドレス、ログバックアップデータのアドレス、およびポイントインタイムリカバリの時刻を解析します。
   * バックアップデータ内のリストア対象オブジェクト（データベースまたはテーブル）を問い合わせ、リストアするテーブルが存在し、リストア要件を満たしているかどうかをチェックします。

2. BRは完全バックアップデータをリストアします。

   * 完全バックアップデータをリストアします。スナップショットバックアップデータのリストアプロセスの詳細については、[スナップショットバックアップデータのリストアのプロセス](/br/br-snapshot-architecture.md#process-of-restore)を参照してください。

3. BRはログバックアップデータをリストアします。

   * **バックアップデータの読み取り**: ログバックアップデータを読み取り、リストアされる必要のあるログバックアップデータを計算します。
   * **リージョン情報の取得**: PDにアクセスしてすべてのリージョンの分布を取得します。
   * **TiKVにデータのリストアを要求**: ログリストアリクエストを作成し、それを対応するTiKVノードに送信します。ログリストアリクエストにはリストアする必要のあるログバックアップデータ情報が含まれます。

4. TiKVはBRからのリストアリクエストを受け入れ、ログリストアワーカーを開始します。

   * ログリストアワーカーはリストアが必要なログバックアップデータを取得します。

5. TiKVはログバックアップデータをリストアします。

   * **KVのダウンロード**: ログリストアワーカーはログリストアリクエストに従ってバックアップストレージから対応するバックアップデータをローカルディレクトリにダウンロードします。
   * **KVの再構築**: ログリストアワーカーは、リストアクラスターテーブルのテーブルIDに従ってバックアップデータのKVデータを書き換えます。つまり、[Key-Value](/tidb-computing.md#mapping-table-data-to-key-value)内の元のテーブルIDを新しいテーブルIDで置換し、同様にインデックスIDも書き換えます。
   * **KVの適用**: ログリストアワーカーは、処理されたKVデータをraftインタフェースを通じてストア（RocksDB）に書き込みます。
   * **リストア結果の報告**: ログリストアワーカーはリストア結果をBRに返します。

6. BRは各TiKVノードからのリストア結果を受け取ります。

   * `RegionNotFound`または`EpochNotMatch`などの理由でリストアに失敗したデータがある場合、例えばTiKVノードがダウンしている場合、BRはリストアを再試行します。
   * リストアに失敗したデータがあり、再試行できない場合、リストアタスクは失敗します。
   * すべてのデータのリストアが完了した後、リストアタスクは成功します。

## ログバックアップファイル

ログバックアップは以下の種類のファイルを生成します:

- `{min_ts}-{uuid}.log`ファイル: バックアップタスクのKV変更ログデータを保存します。`{min_ts}`はファイル内のKV変更ログデータの最小TSOタイムスタンプであり、`{uuid}`はファイル作成時にランダムに生成されます。
- `{checkpoint_ts}-{uuid}.meta`ファイル: 各TiKVノードがログバックアップデータをアップロードするたびに生成され、今回アップロードされたすべてのログバックアップデータファイルのメタデータを保存します。`{checkpoint_ts}`はTiKVノードのログバックアップチェックポイントです。グローバルチェックポイントは全TiKVノードの最小チェックポイントです。`{uuid}`はファイル作成時にランダムに生成されます。
- `{store_id}.ts`ファイル: 各TiKVノードがログバックアップデータをアップロードするたびにグローバルチェックポイントTSで更新されます。`{store_id}`はTiKVノードのストアIDです。
- `v1_stream_truncate_safepoint.txt`ファイル: `br log truncate`によって削除されたストレージ内の最新のバックアップデータに対応するタイムスタンプを保存します。

### バックアップファイルの構造

```
.
├── v1
│   ├── backupmeta
│   │   ├── {min_restored_ts}-{uuid}.meta
│   │   ├── {checkpoint}-{uuid}.meta
│   ├── global_checkpoint
│   │   ├── {store_id}.ts
│   ├── {date}
│   │   ├── {hour}
│   │   │   ├── {store_id}
│   │   │   │   ├── {min_ts}-{uuid}.log
│   │   │   │   ├── {min_ts}-{uuid}.log
├── v1_stream_truncate_safepoint.txt
```

以下は例です:

```
.
├── v1
│   ├── backupmeta
│   │   ├── ...
│   │   ├── 435213818858112001-e2569bda-a75a-4411-88de-f469b49d6256.meta
│   │   ├── 435214043785779202-1780f291-3b8a-455e-a31d-8a1302c43ead.meta
│   │   ├── 435214443785779202-224f1408-fff5-445f-8e41-ca4fcfbd2a67.meta
│   ├── global_checkpoint
│   │   ├── 1.ts
│   │   ├── 2.ts
│   │   ├── 3.ts
│   ├── 20220811
│   │   ├── 03
│   │   │   ├── 1
│   │   │   │   ├── ...
│   │   │   │   ├── 435213866703257604-60fcbdb6-8f55-4098-b3e7-2ce604dafe54.log
│   │   │   │   ├── 435214023989657606-72ce65ff-1fa8-4705-9fd9-cb4a1e803a56.log
│   │   │   ├── 2
│   │   │   │   ├── ...
│   │   │   │   ├── 435214102632857605-11deba64-beff-4414-bc9c-7a161b6fb22c.log
│   │   │   │   ├── 435214417205657604-e6980303-cbaa-4629-a863-1e745d7b8aed.log
│   │   │   ├── 3
│   │   │   │   ├── ...
│   │   │   │   ├── 435214495848857605-7bf65e92-8c43-427e-b81e-f0050bd40be0.log
│   │   │   │   ├── 435214574492057604-80d3b15e-3d9f-4b0c-b133-87ed3f6b2697.log
├── v1_stream_truncate_safepoint.txt