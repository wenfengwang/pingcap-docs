---
title: TiKVの概要
summary：TiKVストレージエンジンの概要。

# TiKVの概要

TiKVは分散およびトランザクションキー値データベースであり、ACID準拠のトランザクションAPIを提供します。[Raftコンセンサスアルゴリズム](https://raft.github.io/raft.pdf)の実装とRocksDBに保存されたコンセンサス状態により、TiKVは複数のレプリカ間のデータ整合性と高可用性を保証します。TiDB分散データベースのストレージ層として、TiKVは読み取りおよび書き込みサービスを提供し、アプリケーションから書き込まれたデータを永続化します。また、TiKVクラスタの統計データを保存します。

## アーキテクチャ概要

TiKVはGoogle Spannerの設計に基づいたマルチraftグループレプリカメカニズムを実装しています。Regionはキー値データ移動の基本単位であり、ストア内のデータ範囲を指します。各Regionは複数のノードにレプリケートされます。これらの複数のレプリカはRaftグループを形成します。RegionのレプリカはPeerと呼ばれます。通常、1つのRegionには3つのPeerがあります。そのうちの1つがリーダーであり、読み取りおよび書き込みサービスを提供します。PDコンポーネントはすべてのRegionを自動的にバランスを保つようにし、TiKVクラスタ内のすべてのノード間で読み取りおよび書き込みスループットが均等であることを保証します。PDと注意深く設計されたRaftグループにより、TiKVは水平スケーラビリティに優れ、簡単に100TB以上のデータを保存できます。

![TiKVアーキテクチャ](/media/tikv-arch.png)

### RegionとRocksDB

各ストア内にRocksDBデータベースがあり、データをローカルディスクに保存します。すべてのRegionデータは、各ストア内の同じRocksDBインスタンスに保存されています。Raftコンセンサスアルゴリズムに使用されるすべてのログは、各ストア内の別のRocksDBインスタンスに保存されています。これは、連続I/OのパフォーマンスがランダムI/Oよりも優れているためです。異なるRocksDBインスタンスがraftログとRegionデータを保存するため、TiKVはraftログとTiKV Regionのすべてのデータ書き込み操作を1つのI/O操作に結合してパフォーマンスを向上させます。

### RegionとRaftコンセンサスアルゴリズム

Regionのレプリカ間のデータ整合性はRaftコンセンサスアルゴリズムによって保証されます。Regionのリーダーだけが書き込みサービスを提供し、Regionの大多数にデータが書き込まれたときにのみ書き込み操作が成功します。

TiKVはクラスタ内の各Regionの適切なサイズを維持しようとします。Regionのサイズは現在デフォルトで96MiBです。このメカニズムは、PDコンポーネントがTiKVクラスタ内のノード間でRegionをバランスさせるのを支援します。Regionのサイズが閾値（デフォルトでは144MiB）を超えると、TiKVは2つ以上のRegionに分割します。Regionのサイズが閾値（デフォルトでは20MiB）よりも小さい場合、TiKVは2つの小さい隣接するRegionを1つのRegionにマージします。

PDが1つのTiKVノードから別のノードにレプリカを移動する場合、まずターゲットノードにLearnerレプリカを追加し、Learnerレプリカのデータがリーダーレプリカのデータとほぼ同じになった後、PDはLearnerレプリカをFollowerレプリカに変更し、ソースノードのFollowerレプリカを削除します。

リーダーレプリカを1つのノードから別のノードに移動する場合、似たようなメカニズムがあります。違いは、LearnerレプリカがFollowerレプリカになった後、Followerレプリカが「リーダートランスファー」操作を行い、自らをリーダーとして選出する選挙を提案する点です。最終的に新しいリーダーは、ソースノードの古いリーダーレプリカを削除します。

## 分散トランザクション

TiKVは分散トランザクションをサポートしています。ユーザー（またはTiDB）は、同じRegionに属するかどうかを心配することなく、複数のキー値ペアを書き込むことができます。TiKVは、ACID制約を実珅するために2相コミットを使用します。詳細については、[TiDBオプティミスティックトランザクションモデル](/optimistic-transaction.md)を参照してください。

## TiKV Coprocessor

TiDBは一部のデータ計算ロジックをTiKV Coprocessorにプッシュします。TiKV Coprocessorは各Regionで計算を処理します。TiKV Coprocessorに送信されるリクエストには、1つのRegionのデータのみが含まれます。